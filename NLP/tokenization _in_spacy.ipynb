{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc72a25-286e-4c85-898d-246a209524b1",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;color:mediumvioletred\">Tokenization in Spacy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc405c57-b57c-4162-8f2a-9ec77f75c859",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73491c4-f8d2-467e-986a-0eebaa31147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7414a9-71b6-43a1-83b7-864bcb8f95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Dr. Strange is a great magician who saved this universe in Spiderman No Way Home movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768476c1-af42-4ca9-be28-7bc469bcf3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "is\n",
      "a\n",
      "great\n",
      "magician\n",
      "who\n",
      "saved\n",
      "this\n",
      "universe\n",
      "in\n",
      "Spiderman\n",
      "No\n",
      "Way\n",
      "Home\n",
      "movie\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9d51c5-6f7f-4a9a-bc75-b14a3f235cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. Strange is a great magician who saved this universe in Spiderman No Way Home movie"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170aa42f-aca2-4d49-911f-91f373a870fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is a great magician"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e71fcf1-7bbf-48f6-8fdd-adf895dc9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(\"Let's go to N.Y.!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a31a2b8-6b95-4152-b72b-1ee4d9a7b994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa1ce13-2c90-4bc6-82dc-c90e3facfe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6afccc0-bf88-4998-b361-392df9fd1b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3484903-8975-4860-b31b-365679293bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2e587a-0246-49c6-b925-8efa8ea3f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(\"Tony gave Peter the special glass of worth two $\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b6644f1-d47f-4fbc-8594-101521ba969f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc3[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7c02da-c164-46a1-b2c3-691e62360eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e27461b-2b58-42f9-85a0-bada670f9066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e9813b3-a772-45e0-a529-b239d2e697b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2e12cd2-56e5-439c-951a-32aa653b33b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token8 = doc3[8]\n",
    "token8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba36ddc0-0ddd-4e24-b692-80a64b9456a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token8.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a51adeb8-2b1f-4fa8-b4fd-ae024fe925c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token9 = doc3[9]\n",
    "token9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0111af0-fe23-4733-81e6-4a735d51ce92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token9.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8203c30-e9dc-424c-82d5-a9e55e6d0289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 Token: 'Tony'  is Alphabetic\n",
      "\n",
      "\n",
      "index: 1 Token: 'gave'  is Alphabetic\n",
      "\n",
      "\n",
      "index: 2 Token: 'Peter'  is Alphabetic\n",
      "\n",
      "\n",
      "index: 3 Token: 'the'  is Alphabetic\n",
      "\n",
      "\n",
      "index: 4 Token: 'special'  is Alphabetic\n",
      "\n",
      "\n",
      "index: 5 Token: 'glass'  is Alphabetic\n",
      "\n",
      "\n",
      "index: 6 Token: 'of'  is Alphabetic\n",
      "\n",
      "\n",
      "index: 7 Token: 'worth'  is Alphabetic\n",
      "\n",
      "\n",
      "index: 8 Token: 'two'  is Alphabetic\n",
      "index: 8 Token: 'two'  is like Numeric\n",
      "\n",
      "\n",
      "index: 9 Token: '$'  is Currency\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    if token.is_alpha:\n",
    "        print(f\"index: {token.i} Token: '{token}'  is Alphabetic\")\n",
    "    if token.is_punct:\n",
    "        print(f\"index: {token.i} Token: '{token}'  is Punctuation\")\n",
    "    if token.like_num:\n",
    "        print(f\"index: {token.i} Token: '{token}'  is like Numeric\")\n",
    "    if token.is_currency:\n",
    "        print(f\"index: {token.i} Token: '{token}'  is Currency\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8d461-3fb1-4faa-84f4-3156c5c4c6ad",
   "metadata": {},
   "source": [
    "## Retrieving links from a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17f2bb1b-73ca-4b8d-b778-709b080220a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a14a62b-37bc-4038-b4a6-a08edca9b947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a60ddcfa-25ca-4ec4-9aa1-90f08361b2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[virat@kohli.com, maria@sharapova.com, serena@williams.com, joe@root.com]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token)\n",
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df4d88-e918-478a-9bad-6bd37f73f785",
   "metadata": {},
   "source": [
    "## Checking other language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "820140ee-c26c-4a13-ae6b-e983412eefc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "আমি আলু দিয়ে মুরগির তরকারি রান্না করছি।"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"bn\")\n",
    "\n",
    "doc = nlp(\"আমি আলু দিয়ে মুরগির তরকারি রান্না করছি।\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17213b6c-2221-486b-9d43-517fe9d33d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমি\n",
      "আলু\n",
      "দিয়ে\n",
      "মুরগির\n",
      "তরকারি\n",
      "রান্না\n",
      "করছি\n",
      "।\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b763a-442a-471d-a4fd-b804cfff0248",
   "metadata": {},
   "source": [
    "## More play with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6eb8eec-c94b-4973-9195-cd1c07bcf640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gimme, double, cheese, extra, large, healthy, pizza]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf039ef5-6484-4a3b-b1d1-5f2947bbaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ddc38ec-54ff-4adb-96b4-ee827ffc309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gim, me, double, cheese, extra, large, healthy, pizza]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6f03fa2-267d-4988-9283-2ad7ce276e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x241bc8dc9c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "031f5e94-b120-4192-b221-30f20526224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64f75f5c-6388-4bdf-8005-9a6675b9f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange loves pizza of pizzaburg.\n",
      "Hulk loves kacchi of Sultan's dine\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pizza of pizzaburg. Hulk loves kacchi of Sultan's dine\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    # for token in sentence:\n",
    "    #     print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3bca3-47e0-48d9-9839-e8ab4f185add",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Exercise</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e1a8e-6416-4423-9a85-b7c72cf595dd",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
    "\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6d4c1c2-b588-4c1b-aa3f-33635d247195",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a92fada-89a1-4668-aa13-d8b0af1bf578",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c91d2c6f-defd-4b8c-857e-18af1d7a998f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[http://www.data.gov/,\n",
       " http://www.science,\n",
       " http://data.gov.uk/.,\n",
       " http://www3.norc.org/gss+website/,\n",
       " http://www.europeansocialsurvey.org/.]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "links = []\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        links.append(token)\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7661f1-27fb-4eec-8cc0-45a137f100f7",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Extract all money transaction from below sentence along with currency. Output should be,\n",
    "    \n",
    "    two $\n",
    "    500 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8dfaee8a-2ae7-424d-9d0a-b378cc99f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c7a22631-9f07-438a-8b04-444242521630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(transactions)\n",
    "results = []\n",
    "tmp = []\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text, doc[token.i+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
